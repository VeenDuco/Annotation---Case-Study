---
title: "Learning From Crowds: Fully Bayesian and Fully Calibrated"
output:
  tufte::tufte_html:
    toc: true
    toc_depth: 2
    
citation_package: natbib
bibliography: ["Annotation Models - project Bob.bib", packages.bib]
---

```{r setup, include=FALSE, echo=FALSE}
packages <- c("ggplot2", "gridExtra", "knitr", "reshape", "rstan",
              "tufte", "cowplot", "bayesplot")
lapply(packages, library, character.only = TRUE)
 knitr::write_bib(c(
   .packages(), packages), 'packages.bib')

options(htmltools.dir.version = FALSE)
options(digits = 2)
knitr::opts_chunk$set(echo = FALSE) # hides everything from chunks but output
knitr::opts_chunk$set(cache = FALSE)
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
knitr::opts_chunk$set(comment = "")
rstan_options(auto_write = TRUE)
# options(mc.cores = parallel::detectCores(logical = FALSE))
source("SIMULATIONS/FUNCTIONS/sbc_rank_hist.r")
source("SIMULATIONS/FUNCTIONS/ggtheme_tufte.r")
```


<!-- notes: -->
<!--  - for annotation examples look at paper from e.g. david blei, image recognition problems. -->

## Abstract {-}
In statistics and machine learning we often consider cases in which there is a dependent variable with known labels and a bunch of predictors that we wish to relate to these labels. It is however not always posible to obtain gold standard labels and instead we might have multiple annotations for each item from different experts or annotators. In this case study we discuss how to relate predictors and annotations of items to one another in a fully Bayesian way. Moreover, we show that specifying a joint annotation and classification model leads to a propperly calibrated model whereas seperating annotation and classification leads to under-dispersed posterior distributions for the relationship between the predictors and the unknown golden standard. 

<hr>
## Annotations

Annotation is concerned with producing gold standard labels [^1]. The problem of producing gold standard labels can be encountered in many differed fields, for instance, Natural Language Processing [@paun_comparing_2019], Sociology (e.g. Cultural consensus theory; @oravecz_bayesian_2014), Item Response Theory [@karabatsos_markov_2003], or to produce labels that can be used in supervised learning [@raykar_learning_2010]. A corpus of annotations usually contains inconsistent labels for items across annotators, and, sometimes inconsistent labels for items from the same annotator that labels the item multiple times. We are interested in figuring out the true classes [^2] for the items. 

It has been well established that probability based models are superior to a simple majority voting scheme to determine the true classes of items [@paun_comparing_2019]. Additionaly, traditional measures of annotator agreement such as Cohen's $\kappa$ and (chance-adjusted) pairwise agreement do not take annotator ability into account when annotators disagree [@passonneau_benefits_2014], which we consider a desirable property for our models. In this case study we use the annotation model of @dawid_maximum_1979 to obtain estimates of annotator sensitivity (true positive rate) and specificity (true negative rate) and true class probabilities. The model of @dawid_maximum_1979 is easily extended to incorporate joint estimation of a classification scheme as shown in @raykar_learning_2010. In contrast to @raykar_learning_2010 we do not use posterior mode estimates, we do a full Bayesian inference and are interested in the calibrartion of our posterior distributions. 


## Calibration 
A well calibrated model means that our probability statements are consistent with long-run observations [@dawid_well-calibrated_1982]. If we use a 60 percent credibility interval, we would indeed find that the predicted quantity is included in this interval in 60 percent of our long-run observations. Recently @talts_validating_2018 described how we could use simulations to test if our models are well calibrated. We provide a brief recap of the idea of Simulation Based Calibration (SBC), for more details see @talts_validating_2018.

First, sample grounded truth values from the prior, $\tilde{\theta} \sim \pi(\theta)$, and thereafter data from the data generating process conditional on these grounded truth values, $\tilde{y} \sim \pi(y|\tilde{\theta})$. Subsequently, do inference on these simulated observations to get the posterior distribution $\pi(\theta|\tilde{y})$. The sampled grounded truth values of the parameters can then be compared to the recovered posterior distributions. Now all the elements are available to make use of the following idea; "*for any model the exact posterior expectation with respect to data generated from the Bayesian joint distribution reduces to the corresponding prior expectation.*"[@talts_validating_2018, p. 4]. Or alternatively written
$$\pi(\theta) = \int d\tilde{y}d\tilde{\theta}\pi(\theta|\tilde{y}) \pi(\tilde{y}|\tilde{\theta})\pi(\tilde{\theta}).$$




SECTION ON CALIBRATION.. 
   - We care about uncertainty and not only about point estimates.
   - Assumption that we want propperly calibrated models that propagate uncertainty throughout. 
   - Briefly explain (simulation based) calibration and that we will use this as a measure to validate our models in the case study. 

In the following we first show that both the annotation model of @dawid_maximum_1979 and the logistics regression that we use for the classification are propperly calibrated, seperately, and, jointly. Thereafter, we show that if you produce a dataset with predictors and golden standard labels based on the annotation model initially and only later run a classification model, you will produce posterior distirbutions that are under-dispersed. 


[^1]: Annotation is alternatively referred to as coding, rating, grading, tagging, or labeling.
[^2]: The true class is alternatively referred to as true label, grounded truth, answer key or golden standard.


<hr>

## Simulation

In these simulatins we restrict ourselves to the binairy classification problem. Extensions to categorical, ordinal and continous data could be considered as in the @raykar_learning_2010 paper. 

### Annotation only model

Assume that we have $J$ annotators which produce annotations for (a subset of) $I$ items so we have a total of $N$ annotations that have $K$ possible labels, which we retrict to $K=2$ here for simplicity. This results in a corpus of annotations $\textbf{y}$ for which we specify probability based models to determine the gold standard label probabilities. We follow the model by Dawid and Skene [-@dawid_maximum_1979] such that 
$$y_{ij} \sim bernoulli(\theta_{j, z_i})$$
$$z_i \sim bernoulli(\pi)$$
$$\alpha_j = \theta_{j, 1}$$
$$\beta_j = 1-\theta_{j,0}$$

where $z_i$ is the true label for item $i$, $\pi = p(z_i = 1)$ [^?], $\alpha_j$ is the annotator sensitivity or the true positive rate such that $\alpha_j = p(y_{ij} = 1 | z_i = 1)$ and $\beta_j$ is the annotator specificity or true negative rate such that $\beta_j = p(y_{ij} = 0 | z_i = 0)$. In the case where we have no additional item information $\pi$ is equal for all $i$ or alternatively stated
$$\pi = logit^{-1}(w_0).$$
[^?]: *can I state it like this?*


### stan code

```{stan, eval = FALSE, output.var="Annotation_only", echo = TRUE}
data {
  int<lower=0> I;
  int<lower=0> J;
  int<lower=0> N;
  int<lower=0,upper=1> y[N];  
  int<lower=1> ii[N];         
  int<lower=1> jj[N];         
}
parameters {
  real w0; 
  vector<lower=0.1,upper=1>[J] alpha;   
  vector<lower=0.1,upper=1>[J] beta;  
}

model {
  vector[I] logit_z_hat;
  vector[I] log_z_hat; 
  vector[I] log1m_z_hat;

  vector[J] log_alpha;
  vector[J] log1m_alpha;
  vector[J] log_beta;
  vector[J] log1m_beta;

  logit_z_hat = rep_vector(w0, I);

  for (i in 1:I) {
    log_z_hat[i] = log_inv_logit(logit_z_hat[i]);
    log1m_z_hat[i] = log1m_inv_logit(logit_z_hat[i]);
  }

  for (j in 1:J) {
    log_alpha[j] = log(alpha[j]);
    log1m_alpha[j] = log1m(alpha[j]);
    log_beta[j] = log(beta[j]);
    log1m_beta[j] = log1m(beta[j]);
  }

  w0 ~ normal(0,5);
  alpha ~ beta(10,1);
  beta ~ beta(10,1);

  for (n in 1:N){
    real pos_sum;
    real neg_sum;
    pos_sum = log_z_hat[ii[n]];
    neg_sum = log1m_z_hat[ii[n]];
    if (y[n] == 1) {
        pos_sum = pos_sum + log_alpha[jj[n]];
        neg_sum = neg_sum + log1m_beta[jj[n]];
      } else {
        pos_sum = pos_sum + log1m_alpha[jj[n]];
        neg_sum = neg_sum + log_beta[jj[n]];
      }
      target += log_sum_exp(pos_sum, neg_sum);
  }

}

```

### Calibration






<hr>
### Classification only model

$$z_i \sim bernoulli(\pi)$$
$$\pi_i = logit^{-1}(w^T X_i)$$

### stan model

```{stan, eval = FALSE, echo = TRUE, output.var = "classification"}
data {
  int<lower=0> I;  
  int<lower=0> N;  
  int<lower=0> D;  
  matrix[I,D] x;   
  int<lower=0,upper=1> y[N];
  int<lower=1> ii[N];       
}

parameters {
  vector[D] w;    
  real w0;        
}

model {
  vector[I] logit_z_hat;
  w ~ normal(0,2);
  w0 ~ normal(0,5);
  
  logit_z_hat = w0 + x * w;
  for(n in 1:N)
    y[n] ~ bernoulli_logit(logit_z_hat[ii[n]]);
}

```




<hr>
## Joint model

$$y_{ij} \sim bernoulli(\theta_{j, z_i})$$
$$z_i \sim bernoulli(\pi)$$
$$\theta_j = (\alpha_j, \beta_j)$$
$$\pi_i = logit^{-1}(w^T X_i)$$

### stan code

```{stan, eval = FALSE, echo = TRUE, output.var = "Joint_model"}
data {
  int<lower=0> I; 
  int<lower=0> J; 
  int<lower=0> N; 
  int<lower=0> D;  // differs from annotation only
  matrix[I,D] x;   // differs from annotation only
  int<lower=0,upper=1> y[N];
  int<lower=1> ii[N];   
  int<lower=1> jj[N];   
}
parameters {
  vector[D] w;   // differs from annotation only
  real w0;      

  vector<lower=0.1,upper=1>[J] alpha; 
  vector<lower=0.1,upper=1>[J] beta;  
}

model {
  vector[I] logit_z_hat;
  vector[I] log_z_hat;   
  vector[I] log1m_z_hat; 

  vector[J] log_alpha;
  vector[J] log1m_alpha;
  vector[J] log_beta;
  vector[J] log1m_beta;

  logit_z_hat = w0 + x * w; // differs from annotation only
  
  for (i in 1:I) {
    log_z_hat[i] = log_inv_logit(logit_z_hat[i]);
    log1m_z_hat[i] = log1m_inv_logit(logit_z_hat[i]);
  }

  for (j in 1:J) {
    log_alpha[j] = log(alpha[j]);
    log1m_alpha[j] = log1m(alpha[j]);
    log_beta[j] = log(beta[j]);
    log1m_beta[j] = log1m(beta[j]);
  }

  w ~ normal(0,2);  // differs from annotation only
  w0 ~ normal(0,5);
  alpha ~ beta(10,1);
  beta ~ beta(10,1);

  for (n in 1:N){
    real pos_sum;
    real neg_sum;
    pos_sum = log_z_hat[ii[n]];
    neg_sum = log1m_z_hat[ii[n]];
    if (y[n] == 1) {
        pos_sum = pos_sum + log_alpha[jj[n]];
        neg_sum = neg_sum + log1m_beta[jj[n]];
      } else {
        pos_sum = pos_sum + log1m_alpha[jj[n]];
        neg_sum = neg_sum + log_beta[jj[n]];
      }
      target += log_sum_exp(pos_sum, neg_sum);
  }

}

```


<hr>
## Concequences of seperation

Imagine a process in which we collect a lot of annotations, via for instance Amazon Mechanical Turk. We "clean" the data so that from all these noisy annotations we get probability based golden standard labels for each item. By this point we are doing better than using majority voting and we create a nice data set with golden standard labels and predictors for all of the items. This data set is then later used to train a classification model once we are intereseted in this data again. We found previously that the joint model is propperly calibrated, but let's look at what happens if we seperate the annotation and the classification problem into two destinct and seperate parts.

Our model becomes: 
$$y_{ij} \sim bernoulli(\theta_{j, z_i})$$
$$z_i \sim bernoulli(\pi)$$
$$\pi = logit^{-1}(w_0)$$
$$\theta_j = (\alpha_j, \beta_j)$$
$$z^*_i = \mathcal{I}[z_i > .5]$$
$$z^*_i \sim bernoulli(\hat{\pi})$$
$$\hat{\pi}_i = logit^{-1}(w^T X_n)$$

### stan model

Now to obtain $\textbf{z}^*$ we add a generated quantities section to the annotation model.

```{stan, eval = FALSE, echo = TRUE, output.var = "gen_quant"}
generated quantities {
  vector[I] E_z;
  {
    vector[I] logit_z_hat;
    vector[I] log_z_hat;   // posterior mean: log Pr[z[n] == 1]
    vector[I] log1m_z_hat; //                 log Pr[z[n] == 0]

    vector[J] log_alpha;
    vector[J] log1m_alpha;
    vector[J] log_beta;
    vector[J] log1m_beta;

    logit_z_hat = rep_vector(w0, I);
    
    for (i in 1:I) {
      log_z_hat[i] = log_inv_logit(logit_z_hat[i]);
      log1m_z_hat[i] = log1m_inv_logit(logit_z_hat[i]);
    }

    for (j in 1:J) {
      log_alpha[j] = log(alpha[j]);
      log1m_alpha[j] = log1m(alpha[j]);
      log_beta[j] = log(beta[j]);
      log1m_beta[j] = log1m(beta[j]);
    }

   for (n in 1:N){
    real pos_sum;
    real neg_sum;
    real maxpn;
    pos_sum = log_z_hat[ii[n]];
    neg_sum = log1m_z_hat[ii[n]];
    if (y[n] == 1) {
        pos_sum = pos_sum + log_alpha[jj[n]];
        neg_sum = neg_sum + log1m_beta[jj[n]];
      } else {
        pos_sum = pos_sum + log1m_alpha[jj[n]];
        neg_sum = neg_sum + log_beta[jj[n]];
      }
    maxpn = fmax(pos_sum, neg_sum);
      pos_sum = pos_sum - maxpn;
      neg_sum = neg_sum - maxpn;
      E_z[ii[n]] = exp(pos_sum) / (exp(pos_sum) + exp(neg_sum));
  }
  }
}

```

As an idicator function we use the following piece of R code:


```{r, eval = FALSE, echo = TRUE}
z_star <- ifelse(summary(fit, pars = "E_z")$summary[, 1] < 0.5, 0, 1)
```


Thereafter we can fit the classification model with $\textbf{z}^*$.

<hr>

# Conclusion / Take Home Message

If you have a corpus of annotations do not "clean-up" this data set by producing golden standard labels. If you care about coverage of your parameters at all model the classification and annotation jointly and propagate your uncertainty throughout. 

<hr>



```{r, include = FALSE}

finalMatrix <- readRDS(paste0("SIMULATIONS/RESULTS/finalMatrixRDS/",
                              "logistic_regressionI100J10D10missing50Bins19replications3000.rds"))
plotList <- apply(finalMatrix, 2, sbc_rank_hist, reps = nrow(finalMatrix), bins = max(finalMatrix) + 1)

n <- length(plotList)
nCol <- floor(sqrt(n))
# do.call("grid.arrange", c(plotList, ncol=nCol, 
#                           top = "SBC logistic regression for Intercept and 10 predictors")) 
g <- do.call("grid.arrange", c(plotList, ncol=nCol, 
                               top = "SBC logistic regression for Intercept and 10 predictors"))
```


````{r}
g2 <- cowplot::ggdraw(g) + ggtheme_tufte()
plot(g2)

```

<!-- `r margin_note("")` -->
<!--
```{marginfigure}
<i>Introduction Outline</i>: <br>
  * Annotation models <br><br>
  * Latent truths <br><br>
  * Relate to other field, e.g. cultural consensus theory, medicine <br><br>
  * Goal of golden standards with uncertainty <br><br>
  * This provides great benefit over majority voting, see literature <br><br>
  * Moreover, if we can obtain golden standard labels with appropriate uncertainty this can be a step towards machine learning with appropriate uncertainty in the labels on which we train our models.<br><br>
  * We start this case study with Model proposed by Dawid and Skene and compare the usage of EM-algorithm (taken from ..) and the Bayesian model (taken from ..).<br><br>
``` 
-->



## Session information
```{r}
sessionInfo()
```

## References

 