---
title: "Learning From Crowds: Fully Bayesian and Fully Calibrated"
output:
  tufte::tufte_html:
    toc: true
    toc_depth: 1
    
citation_package: natbib
bibliography: ["Annotation Models - project Bob.bib", packages.bib]
---

```{r setup, include=FALSE, echo=FALSE}
packages <- c("ggplot2", "gridExtra", "knitr", "reshape", "rstan",
              "tufte", "cowplot", "bayesplot")
lapply(packages, library, character.only = TRUE)
 knitr::write_bib(c(
   .packages(), packages), 'packages.bib')

options(htmltools.dir.version = FALSE)
options(digits = 2)
knitr::opts_chunk$set(echo = FALSE) # hides everything from chunks but output
knitr::opts_chunk$set(cache = FALSE)
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
knitr::opts_chunk$set(comment = "")
rstan_options(auto_write = TRUE)
# options(mc.cores = parallel::detectCores(logical = FALSE))
source("SIMULATIONS/FUNCTIONS/sbc_rank_hist.r")
source("SIMULATIONS/FUNCTIONS/ggtheme_tufte.r")
```


## Introduction


```{r, include = FALSE}

finalMatrix <- readRDS(paste0("SIMULATIONS/RESULTS/finalMatrixRDS/",
                              "logistic_regressionI100J10D10missing50Bins19replications3000.rds"))
plotList <- apply(finalMatrix, 2, sbc_rank_hist, reps = nrow(finalMatrix), bins = max(finalMatrix) + 1)

n <- length(plotList)
nCol <- floor(sqrt(n))
# do.call("grid.arrange", c(plotList, ncol=nCol, 
#                           top = "SBC logistic regression for Intercept and 10 predictors")) 
g <- do.call("grid.arrange", c(plotList, ncol=nCol, 
                               top = "SBC logistic regression for Intercept and 10 predictors"))
```

````{r}
g2 <- cowplot::ggdraw(g) + ggtheme_tufte()
plot(g2)

```

<!-- `r margin_note("")` -->

```{marginfigure}
<i>Introduction Outline</i>: <br>
  * Annotation models <br><br>
  * Latent truths <br><br>
  * Relate to other field, e.g. cultural consensus theory, medicine <br><br>
  * Goal of golden standards with uncertainty <br><br>
  * This provides great benefit over majority voting, see literature <br><br>
  * Moreover, if we can obtain golden standard labels with appropriate uncertainty this can be a step towards machine learning with appropriate uncertainty in the labels on which we train our models.<br><br>
  * We start this case study with Model proposed by Dawid and Skene and compare the usage of EM-algorithm (taken from ..) and the Bayesian model (taken from ..).<br><br>
``` 

In this case study we will discuss models of annotation. Annotation is concerned with producing gold standard labels and alternatively referred to as coding, rating, grading, tagging, or labeling. The problem of producing gold standard labels can be encountered in many differed fields, for instance, Natural Language Processing (NLP) [@paun_comparing_2019], Medicine (refs, radiology examples), Sociology (?, I refer to Cultural consensus theory) [@oravecz_bayesian_2014], Item Response Theory (IRT) [@karabatsos_markov_2003], or to produce labels that can be used in a supervised learning setting [@paun_comparing_2019].

The common setting is that we have $J$ annotators which produce labels for (a subset of) $I$ items which have $K$ possible categories. In total there are $N$ annotations. We assume that the true class ($c$) (aka. true label, grounded truth, answer key, golden standard) is unknown. There exist class prevalence's ($\pi$), annotators have abilities ($\beta$) and items have a difficulty ($\theta$). The $K$ categories can be dichotomous or polytomous, with a nominal or ordinal scale. The items can be allowed to have continuous scales too, yet for now we restrict ourselves to the categorical case. 

The most simple and straightforward way to define a golden standard would be to use a majority voting scheme in which for each item $i \in \{1,2,...,I \}$ the label is taken that was chosen by the majority of the annotators. Majority voting and the associated judgments regarding annotators have been shown to be inferior to model based approaches [@paun_comparing_2019]. We therefore start this case study with the model proposed by @dawid_maximum_1979 that allows model based estimation of golden standard labels and associated annotator biases. Note that this model does not include item difficulty estimation. We compare their original approach, using an EM algorithm, to using a full Bayesian model. We analyze the original data from the @dawid_maximum_1979 paper and conduct a simulation study to investigate systematically which approach leads to more accurate results. 


## Case study

### Dawid and Skene model


### Data from Dawid and Skene
The case study of @dawid_maximum_1979 concerns the pre-operative assessment of patient fitness to undergo general anesthetics. 45 patients are assesed and rated on a 1 to 4 scale such that $I = 45$ and $K = 4$. There are five annotators, four of which rate each patient once, and one of which annotates each patient three time, as such $J = 5$ and $N = 315$. As a simple consensus could not be found on the basis of the ratings for each patient^[Visual representation of the data from @dawid_maximum_1979.].





## Session information
```{r}
sessionInfo()
```

## References

 