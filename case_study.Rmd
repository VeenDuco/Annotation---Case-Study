---
title: "Learning From Crowds: Fully Bayesian and Fully Calibrated"
output:
  tufte::tufte_html:
    toc: true
    toc_depth: 1
    
citation_package: natbib
bibliography: ["Annotation Models - project Bob.bib", packages.bib]
---

```{r setup, include=FALSE, echo=FALSE}
packages <- c("ggplot2", "gridExtra", "knitr", "reshape", "rstan",
              "tufte", "cowplot", "bayesplot")
lapply(packages, library, character.only = TRUE)
 knitr::write_bib(c(
   .packages(), packages), 'packages.bib')

options(htmltools.dir.version = FALSE)
options(digits = 2)
knitr::opts_chunk$set(echo = FALSE) # hides everything from chunks but output
knitr::opts_chunk$set(cache = FALSE)
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
knitr::opts_chunk$set(comment = "")
rstan_options(auto_write = TRUE)
# options(mc.cores = parallel::detectCores(logical = FALSE))
source("SIMULATIONS/FUNCTIONS/sbc_rank_hist.r")
source("SIMULATIONS/FUNCTIONS/ggtheme_tufte.r")
```


<!-- notes: -->
<!--  - for annotation examples look at paper from e.g. david blei, image recognition problems. -->

## Abstract {-}

<hr>

In this case study we will discuss models of annotation. Annotation is concerned with producing gold standard labels [^1]. The problem of producing gold standard labels can be encountered in many differed fields, for instance, Natural Language Processing [@paun_comparing_2019], Medicine (refs, radiology examples), Sociology (?, I refer to Cultural consensus theory) [@oravecz_bayesian_2014], Item Response Theory [@karabatsos_markov_2003], or to produce labels that can be used in supervised learning [@raykar_learning_2010]. The common setting is that we have $J$ annotators which produce labels for (a subset of) $I$ items which have $K$ possible categories. This results in $\textbf{Y}$, a corpus of observed annotations. We are interested in figuring out the true classes [^2] ($\textbf{Z}$) for the items, either because we want to determine a course of action based on the true class (e.g. in Medicine based on a diagnosis) or we want to relate predictors ($\textbf{X}$) to the true classes. 

A corpus of annotations usually contain inconsistent labels for items across annotators, and, sometimes even inconsistent labels for items from the same annotator that labels the item multiple times. It has been well established that probability based models are superior to a simple majority voting scheme to determine the true classes of items [@paun_comparing_2019]. Additionaly, traditional measures of annotator agreement such as Cohen's $\kappa$ and (chance-adjusted) pairwise agreement do not take annotator ability into account when annotators disagree [@passonneau_benefits_2014]. 

 - Paragraph to preview what is to come. 
 - we shall discuss a fully Bayesian way to: 
   - Incorporate annotator ability ($\theta$) in determination of $\textbf{Z}$
   - Relate $\textbf{X}$ to $\textbf{Z}$ 
   - Do all of this in a propperly calibrated way
   
<!-- In this case study we consider a fully Bayesian model that allows the incorporation of annotator abilities ($\theta$) in the determiniation of $p(z_i = k)$ such that we get $p(z | y, \theta)$. Moreover, we want to consider a fully Bayesian models that propperly  -->

[^1]: Annotation is alternatively referred to as coding, rating, grading, tagging, or labeling.
[^2]: The true class is alternatively referred to as true label, grounded truth, answer key or golden standard.

<hr>
# Calibration

  - We care about uncertainty and not only about point estimates.
  - Assumption that we want propperly calibrated models that propagate uncertainty throughout. 
  - Briefly explain (simulation based) calibration and that we will use this as a measure to validate our models in the case study. 

<hr>
# Annotation model



<hr>
<hr>



```{r, include = FALSE}

finalMatrix <- readRDS(paste0("SIMULATIONS/RESULTS/finalMatrixRDS/",
                              "logistic_regressionI100J10D10missing50Bins19replications3000.rds"))
plotList <- apply(finalMatrix, 2, sbc_rank_hist, reps = nrow(finalMatrix), bins = max(finalMatrix) + 1)

n <- length(plotList)
nCol <- floor(sqrt(n))
# do.call("grid.arrange", c(plotList, ncol=nCol, 
#                           top = "SBC logistic regression for Intercept and 10 predictors")) 
g <- do.call("grid.arrange", c(plotList, ncol=nCol, 
                               top = "SBC logistic regression for Intercept and 10 predictors"))
```

````{r}
g2 <- cowplot::ggdraw(g) + ggtheme_tufte()
plot(g2)

```

<!-- `r margin_note("")` -->
<!--
```{marginfigure}
<i>Introduction Outline</i>: <br>
  * Annotation models <br><br>
  * Latent truths <br><br>
  * Relate to other field, e.g. cultural consensus theory, medicine <br><br>
  * Goal of golden standards with uncertainty <br><br>
  * This provides great benefit over majority voting, see literature <br><br>
  * Moreover, if we can obtain golden standard labels with appropriate uncertainty this can be a step towards machine learning with appropriate uncertainty in the labels on which we train our models.<br><br>
  * We start this case study with Model proposed by Dawid and Skene and compare the usage of EM-algorithm (taken from ..) and the Bayesian model (taken from ..).<br><br>
``` 
-->



## Session information
```{r}
sessionInfo()
```

## References

 